{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Model Serving\n",
    "\n",
    "In this notebook, you'll deploy an ONNX model to Triton Inference Server and run inference on it.\n",
    "\n",
    "**[4.1 Learning Objectives](#4.1-Learning-Objectives)<br>**\n",
    "**[4.2 Set Up Triton Server](#4.2-Set-Up-Triton-Server)<br>**\n",
    "**[4.3 Query Model](#4.3-Query-Model)<br>**\n",
    "**[4.4 Run Inference](#4.4-Run-Inference)<br>**\n",
    "**[4.5 Conclusion](#4.5-Conclusion)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4.1 Learning Objectives\n",
    "\n",
    "Centralized model serving can be a huge design win for your business products and/or applications. Hosting models in a central location reduces memory usage and can be designed to reduce inter-device communication.\n",
    "This example will use [NVIDIA's Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) to serve the model exported in the previous section. \n",
    "\n",
    ">Triton Inference Server, part of the NVIDIA AI platform, streamlines and standardizes AI inference by enabling teams to deploy, run, and scale trained AI models from any framework on any GPU- or CPU-based infrastructure. It provides AI researchers and data scientists the freedom to choose the right framework for their projects without impacting production deployment. It also helps developers deliver high-performance inference across cloud, on-prem, edge, and embedded devices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <video controls src=\"https://dli-lms.s3.amazonaws.com/assets/s-ov-10-v1/DLI_part_7.mp4\" width=800 >\n",
    "           type=\"video/mp4\"\n",
    "           width=800>\n",
    "    </video>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4.2 Set Up Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p /opt/model_repository/ # create the folder for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Triton Inference Server\n",
    "import subprocess\n",
    "server = subprocess.Popen([\"tritonserver\", \"--model-repository=/opt/model_repository/\", \"--model-control-mode=poll\", \"--http-port=8988\"])\n",
    "server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice feature of Triton is the [ability to have it \"poll\" a model repository](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_management.md#model-control-mode-poll) to see if a change has occurred. So all that needs to be done is copy the model into the `model_repository` directory. You can read more on the specifics [here](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#repository-layout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /opt/model_repository/our_new_model/ # create the folder with the model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /opt/model_repository/our_new_model/1/ # create the folder with the model name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we copy our model into the repository. An example ONNX model is provided and copied in the next cell.  If you would like to use the one you created, uncomment and run the second line instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /dli/task/data/model.onnx /opt/model_repository/our_new_model/1/model.onnx # move the file to the directory\n",
    "#!cp /dli/task/data/custom_model.onnx /opt/model_repository/our_new_model/1/model.onnx # Delete above line and run this is you want to use you ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 4.3 Query Model\n",
    "\n",
    "Now that the model is in Triton, it has automatically created a model config and loaded it. Let's query it to find out more!\n",
    "\n",
    "_Note: if executing this cell results in an error, this may be because the polling has not \"found\" the model yet.  Wait a few seconds and run the cell again._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "inference_server_url = \"localhost:8001\"\n",
    "triton_client = grpcclient.InferenceServerClient(url=inference_server_url)\n",
    "\n",
    "# find out info about model\n",
    "model_name = \"our_new_model\"\n",
    "triton_client.get_model_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also create a custom config to control other parameters like batch size or maximum number of requests.\n",
    "\n",
    "However, now we are going to do our inference with the model!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4.4 Run Inference\n",
    "\n",
    "You can see in the config above we have the input and output names of the model. Let's use this information to do inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritonclient.utils import triton_to_np_dtype\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# load image data\n",
    "target_width, target_height = 1024, 1024\n",
    "image_bgr = cv2.imread(\"sample_image.png\")\n",
    "image_bgr = cv2.resize(image_bgr, (target_width, target_height))\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "image = np.float32(image_rgb)\n",
    "\n",
    "# preprocessing\n",
    "image = image/255\n",
    "image = np.moveaxis(image, -1, 0)  # HWC to CHW\n",
    "    \n",
    "image = image[np.newaxis, :] # add batch dimension\n",
    "image = np.float32(image)\n",
    "\n",
    "plt.imshow(image_rgb)\n",
    "\n",
    "# create input\n",
    "input_name = \"input\"\n",
    "inputs = [grpcclient.InferInput(input_name, image.shape, \"FP32\")]\n",
    "inputs[0].set_data_from_numpy(image)\n",
    "\n",
    "output_names = [\"boxes\", \"labels\", \"scores\"]\n",
    "outputs = [grpcclient.InferRequestedOutput(n) for n in output_names]\n",
    "\n",
    "\n",
    "results = triton_client.infer(model_name, inputs, outputs=outputs)\n",
    "\n",
    "boxes, labels, scores = [results.as_numpy(o) for o in output_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# annotate\n",
    "annotated_image = image_bgr.copy()\n",
    "            \n",
    "if boxes.size > 0:  # ensure something is found\n",
    "    for box, lab, scr in zip(boxes, labels, scores):\n",
    "\n",
    "        if scr > 0.2:\n",
    "            box_top_left = int(box[0]), int(box[1])\n",
    "            box_bottom_right = int(box[2]), int(box[3])\n",
    "            text_origin = int(box[0]), int(box[3])\n",
    "\n",
    "            border_color = (50, 0, 100)\n",
    "            text_color = (255, 255, 255)\n",
    "\n",
    "            font_scale = 0.9\n",
    "            thickness = 1\n",
    "\n",
    "            # bounding box2\n",
    "            cv2.rectangle(annotated_image, box_top_left, box_bottom_right, border_color, thickness=5,\n",
    "                          lineType=cv2.LINE_8)\n",
    "        \n",
    "plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the Triton server\n",
    "! kill $(pidof tritonserver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4.5 Conclusion\n",
    "\n",
    "You've completed the course - great job! <br>\n",
    "Once you are ready, conclude by watching the final video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <video controls \n",
    "           src=\"https://dli-lms.s3.amazonaws.com/assets/s-ov-10-v1/DLI_part_8.mp4\"\n",
    "           type=\"video/mp4\"\n",
    "           width=800>\n",
    "    </video>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this final notebook, you have:\n",
    "- Deployed an ONNX model to Triton Inference Server\n",
    "- Used Triton to run inference with your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
