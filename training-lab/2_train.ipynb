{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554444a8",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1cc7ff",
   "metadata": {},
   "source": [
    "# 2.0 Fine-Tune Pretrained Model with Your Dataset\n",
    "\n",
    "In this notebook, you'll fine-tune a pretrained model with your synthetic data that is ready for deployment.\n",
    "\n",
    "**[2.1 Learning Objectives](#2.1-Learning-Objectives)<br>**\n",
    "**[2.2 Getting Our Data Ready](#2.2-Getting-Our-Data-Ready)<br>**\n",
    "**[2.3 Create Class](#2.3-Create-Class)<br>**\n",
    "**[2.4 Create Helper Functions](#2.4-Create-Helper-Functions)<br>**\n",
    "**[2.5 Create Model and Train](#2.5-Create-Model-and-Train)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a814e-55b2-47e3-b353-e9a9de1c3a3e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.1 Learning Objectives\n",
    "\n",
    "Training on synthetic data works the same as training on real data. You can plug your new custom synthetic dataset into your existing training workflow.\n",
    "\n",
    "In this example, we will train our data over an object detection model from Torchvision, [`fasterrcnn_resnet50`](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fastercnn_resnet50_fpn.html).  The model has been _pretrained_ on an Imagenet dataset, meaning it already is able to recognize a group of objects.  Our goal is to further train the model, or _fine-tune_ it, with our own custom fruit dataset, so that it will recognize our custom objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0150ff9-c605-4b03-8322-e1081284784e",
   "metadata": {},
   "source": [
    "<center><video controls src=\"https://dli-lms.s3.amazonaws.com/assets/s-ov-10-v1/DLI_part_5.mp4\" width=800 ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7c647-8bdf-4afa-a0b1-75a087020eee",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Getting Our Data Ready\n",
    "\n",
    "We do some preliminary prep on our synthetic data to get it ready for the model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ca59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import transforms as T\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82beca-cbb0-4901-a4a2-1345856b8e9a",
   "metadata": {},
   "source": [
    "We start by defining the epochs and classes for our training script. We have 10 total fruits we wish to identify in the images and will start by running the training for 15 epochs. After you are done with the initial training, feel free to change the number of epochs to see how it changes your loss function value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa7120-14b4-48aa-9574-caccda827569",
   "metadata": {},
   "source": [
    "We can navigate to our data that we generated by opening up a terminal in our JupyterLab window. Press the \"+\" button in the top left to access a new terminal. Our data generation script defaults to `/dli/task/data/fruit_data_$DATE`. For now, we have the data directory set to an example dataset you may choose to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b0216",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/dli/task/data/fruit_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3cf09e-0ac8-4cba-8d7e-9a0a4e47c0f7",
   "metadata": {},
   "source": [
    "Next, we define our output directory, which is where we will save our PyTorch model. There is also an example model saved to `/dli/task/data/model.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ab0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/dli/task/model.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5395cb-2787-4e58-8174-de190ef8c877",
   "metadata": {},
   "source": [
    "In our system today, we are using an NVIDIA GPU. This gives us a powerful compute engine for training and state of the art tech for our graphics applications as well. Run the next cell to see the specs for the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52946a-6730-473e-aee0-b3be4f56137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af1ac6-9124-4d4d-9a0b-12c5772a2d76",
   "metadata": {},
   "source": [
    "We define our device for the training to make sure to use the GPU we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ed0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da60f03f-820f-415b-871a-06f7ecc49e52",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 Create Class\n",
    "\n",
    "In the next cell, we define our `FruitDataset` class and the data loader for the training. In the class definition, there are comments to explain each step.  Please review these and the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitDataset(torch.utils.data.Dataset):\n",
    "    # This function is run once when instantiating the Dataset object\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # In the first portion of this code we are taking our single dataset folder \n",
    "        # and splitting it into three folders based on the file types.\n",
    "        # This is just a preprocessing step.\n",
    "        list_ = os.listdir(root)\n",
    "        for file_ in list_:\n",
    "            name, ext = os.path.splitext(file_)\n",
    "            ext = ext[1:]\n",
    "            if ext == '':\n",
    "                continue\n",
    "\n",
    "            if os.path.exists(root+ '/' + ext):\n",
    "                shutil.move(root+'/'+file_, root+'/'+ext+'/'+file_)\n",
    "\n",
    "            else:\n",
    "                os.makedirs(root+'/'+ext)\n",
    "                shutil.move(root+'/'+file_, root+'/'+ext+'/'+file_)\n",
    "\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"png\"))))\n",
    "        self.label = list(sorted(os.listdir(os.path.join(root, \"json\"))))\n",
    "        self.box = list(sorted(os.listdir(os.path.join(root, \"npy\"))))\n",
    "        # We have our three attributes with the img, label, and box data\n",
    "\n",
    "    # Loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"png\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label_path = os.path.join(self.root, \"json\", self.label[idx])\n",
    "\n",
    "        with open(os.path.join('root', label_path), \"r\") as json_data:\n",
    "            json_labels = json.load(json_data)\n",
    "        \n",
    "        box_path = os.path.join(self.root, \"npy\", self.box[idx])\n",
    "        dat = np.load(str(box_path))   \n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in dat:\n",
    "            obj_val = i[0]\n",
    "            xmin = torch.as_tensor(np.min(i[1]), dtype=torch.float32)\n",
    "            xmax = torch.as_tensor(np.max(i[3]), dtype=torch.float32)\n",
    "            ymin = torch.as_tensor(np.min(i[2]), dtype=torch.float32)\n",
    "            ymax = torch.as_tensor(np.max(i[4]), dtype=torch.float32)\n",
    "            if (ymax > ymin) & (xmax > xmin):\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                area = (xmax - xmin) * (ymax - ymin)\n",
    "            labels += [json_labels.get(str(obj_val)).get('class')]\n",
    "\n",
    "        label_dict = {}\n",
    "\n",
    "        # Labels for the dataset\n",
    "        static_labels = {\n",
    "            'apple' : 0,\n",
    "            'avocado' : 1,\n",
    "            'kiwi' : 2,\n",
    "            'lime' : 3,\n",
    "            'lychee' : 4,\n",
    "            'pomegranate' : 5,\n",
    "            'onion' : 6,\n",
    "            'strawberry' : 7,\n",
    "            'lemon' : 8,\n",
    "            'orange' : 9\n",
    "        }\n",
    "\n",
    "        labels_out = []\n",
    "        # Transforming the input labels into a static label dictionary to use\n",
    "        for i in range(len(labels)):\n",
    "            label_dict[i] = labels[i]\n",
    "\n",
    "        for i in label_dict:\n",
    "            fruit = label_dict[i]\n",
    "            final_fruit_label = static_labels[fruit]\n",
    "            labels_out += [final_fruit_label]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(labels_out, dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.tensor([idx]) \n",
    "        target[\"area\"] = area\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img= self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "    # Finally we have a function for the number of samples in our dataset\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03126d04-2a5e-48c3-85ad-4412e7a2c1b9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.4 Create Helper Functions \n",
    "\n",
    "Next, we define a function for the feature and label transformations we wish to perform. We are converting to `Tensor` objects and also converting the `dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b8b5f-c0c2-4cc0-985b-c37b45b25aa1",
   "metadata": {},
   "source": [
    "Create a function to collate our samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc8acb-257c-4d3f-ab91-c5a5849c67f8",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.5 Create Model and Train\n",
    "\n",
    "Next, we go through the process of actually creating our model. We are starting with the pretrained (default weights) object detection `fasterrcnn_resnet50` model from Torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes): \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa97d0c-a404-47e6-9493-c5d6327e34c9",
   "metadata": {},
   "source": [
    "At this point, we are ready to create our dataset by using our custom `FruitDataset` class and our synthetic data. This is then passed into our `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FruitDataset(data_dir, get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "dataset, batch_size=16, shuffle=True, collate_fn= collate_fn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d7436-f2eb-4d06-8cfc-63a8c210a094",
   "metadata": {},
   "source": [
    "Next, we create our model with the 10 classes we have of fruit and transfer it to the GPU for training. We use [PyTorch SDG](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) (stochastic gradient descent) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e08427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_classes)\n",
    "model.to(device)\n",
    "    \n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001)\n",
    "len_dataloader = len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1ed0f-72a7-4312-b17a-565f6f0efc85",
   "metadata": {},
   "source": [
    "Now we can actually train our model. We keep track of our loss and print it out as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e828acb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "ep = 0\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    ep += 1\n",
    "    i = 0    \n",
    "    for imgs, annotations in data_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {ep} Iteration: {i}/{len_dataloader}, Loss: {losses}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446ad38-4df4-4f87-b132-ff140a67ba7f",
   "metadata": {},
   "source": [
    "Our final step is to save the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac90100-dff8-4dcf-9c1a-4fb15272cfce",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Set up a class for your dataset and data loader\n",
    "- Used your synthetic data to train a model\n",
    "- Saved your model\n",
    "\n",
    "Move on to the [Export notebook](3_export.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae3956",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
